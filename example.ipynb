{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5af8aa",
   "metadata": {},
   "source": [
    "# First we import our CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = CIFAR10(root=DATA_DIR, train=True, transform=transforms.Compose([transforms.ToTensor()]), download=True)\n",
    "test_dataset = CIFAR10(root=DATA_DIR, train=False, transform=transforms.Compose([transforms.ToTensor()]), download=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "num_train_images = len(train_dataset)\n",
    "num_test_images = len(test_dataset)\n",
    "\n",
    "print(f\"Loaded batches of size {BATCH_SIZE}:\")\n",
    "print(f\" - {len(train_dataloader)} batches for training ({num_train_images} images)\")\n",
    "print(f\" - {len(test_dataloader)} batches for validation ({num_test_images} images)\")\n",
    "print(f\"for a total of {num_train_images + num_test_images} images (Shape: {train_dataset[0][0].shape}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72989229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_some_images(num_images, num_rows, dataloader, classes):\n",
    "    num_cols = num_images // num_rows\n",
    "\n",
    "    images, labels = next(iter(dataloader))\n",
    "    plt.figure(figsize=(num_cols, num_rows))\n",
    "    for i in range(num_images):\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(classes[labels[i].item()])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_some_images(16, 2, train_dataloader, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e40bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090caec",
   "metadata": {},
   "source": [
    "# Model Implementation - Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def zero_init(module: nn.Module) -> nn.Module:\n",
    "    \"\"\"Initialize module parameters to zero.\"\"\"\n",
    "    for p in module.parameters():\n",
    "        nn.init.zeros_(p.data)\n",
    "    return module\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim: int, dtype=torch.float32, max_timescale=10_000, min_timescale=1):\n",
    "    \"\"\"Create sinusoidal timestep embeddings.\"\"\"\n",
    "    assert timesteps.ndim == 1\n",
    "    assert embedding_dim % 2 == 0\n",
    "    timesteps *= 1000.0\n",
    "    num_timescales = embedding_dim // 2\n",
    "    inv_timescales = torch.logspace(\n",
    "        -np.log10(min_timescale),\n",
    "        -np.log10(max_timescale),\n",
    "        num_timescales,\n",
    "        device=timesteps.device,\n",
    "    )\n",
    "    emb = timesteps.to(dtype)[:, None] * inv_timescales[None, :]\n",
    "    return torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "\n",
    "\n",
    "def fourier_encode(x: torch.Tensor, num_frequencies: int = 7) -> torch.Tensor:\n",
    "    \"\"\"Apply Fourier feature encoding to input.\"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    device = x.device\n",
    "    dtype = x.dtype\n",
    "    n = torch.arange(num_frequencies, device=device, dtype=dtype)\n",
    "    freqs = (2.0**n) * (2.0 * math.pi)\n",
    "    angles = x.unsqueeze(2) * freqs.view(1, 1, -1, 1, 1)\n",
    "    sin_feats = torch.sin(angles).reshape(B, C * num_frequencies, H, W)\n",
    "    cos_feats = torch.cos(angles).reshape(B, C * num_frequencies, H, W)\n",
    "    return torch.cat([x, sin_feats, cos_feats], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b22d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Block\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=None, condition_dim=None, norm_num_groups=32):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels or in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.condition_dim = condition_dim\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.GroupNorm(norm_num_groups, in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.GroupNorm(norm_num_groups, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        if condition_dim is not None:\n",
    "            self.cond_proj = zero_init(nn.Linear(condition_dim, out_channels))\n",
    "        else:\n",
    "            self.cond_proj = None\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        h = self.net1(x)\n",
    "        if self.cond_proj is not None:\n",
    "            condition = self.cond_proj(condition)\n",
    "            condition = condition[:, :, None, None]\n",
    "            h = h + condition\n",
    "\n",
    "        h = self.net2(h)\n",
    "        if x.shape[1] != self.out_channels:\n",
    "            x = self.shortcut_conv(x)\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Block\n",
    "def attention_inner_heads(qkv, num_heads):\n",
    "    \"\"\"Computes attention with heads inside of qkv in the channel dimension.\"\"\"\n",
    "    bs, width, length = qkv.shape\n",
    "    ch = width // (3 * num_heads)\n",
    "    q, k, v = qkv.chunk(3, dim=1)\n",
    "    scale = ch ** (-1 / 4)\n",
    "    q = q * scale\n",
    "    k = k * scale\n",
    "    new_shape = (bs * num_heads, ch, length)\n",
    "    q = q.view(*new_shape)\n",
    "    k = k.view(*new_shape)\n",
    "    v = v.reshape(*new_shape)\n",
    "    weight = torch.einsum(\"bct,bcs->bts\", q, k)\n",
    "    weight = F.softmax(weight.float(), dim=-1).to(weight.dtype)\n",
    "    out = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "    return out.reshape(bs, num_heads * ch, length)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        spatial_dims = qkv.shape[2:]\n",
    "        qkv = qkv.view(*qkv.shape[:2], -1)\n",
    "        out = attention_inner_heads(qkv, self.n_heads)\n",
    "        return out.view(*out.shape[:2], *spatial_dims)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_heads, n_channels, norm_groups):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=norm_groups, num_channels=n_channels),\n",
    "            nn.Conv2d(n_channels, 3 * n_channels, kernel_size=1),\n",
    "            Attention(n_heads),\n",
    "            zero_init(nn.Conv2d(n_channels, n_channels, kernel_size=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up/Down Block\n",
    "class UpDownBlock(nn.Module):\n",
    "    def __init__(self, resnet_block, attention_block=None):\n",
    "        super().__init__()\n",
    "        self.resnet_block = resnet_block\n",
    "        self.attention_block = attention_block\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        x = self.resnet_block(x, cond)\n",
    "        if self.attention_block is not None:\n",
    "            x = self.attention_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, n_blocks=32, input_channels=3, use_fourier=True, num_fourier_features=7):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_blocks = n_blocks\n",
    "        self.use_fourier = use_fourier\n",
    "        self.num_fourier_features = num_fourier_features\n",
    "        self.gamma_min = -13.3\n",
    "        self.gamma_max = 5.0\n",
    "\n",
    "        attention_params = dict(n_heads=1, n_channels=embedding_dim, norm_groups=32)\n",
    "        resnet_params = dict(\n",
    "            in_channels=embedding_dim, out_channels=embedding_dim, condition_dim=4 * embedding_dim, norm_num_groups=32\n",
    "        )\n",
    "\n",
    "        self.embed_conditioning = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim * 4),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        total_input_ch = input_channels * (1 + 2 * num_fourier_features) if use_fourier else input_channels\n",
    "        self.input_conv = nn.Conv2d(total_input_ch, embedding_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([UpDownBlock(ResnetBlock(**resnet_params)) for _ in range(n_blocks)])\n",
    "\n",
    "        self.mid_resnet_block_1 = ResnetBlock(**resnet_params)\n",
    "        self.mid_attn_block = AttentionBlock(**attention_params)\n",
    "        self.mid_resnet_block_2 = ResnetBlock(**resnet_params)\n",
    "\n",
    "        resnet_params[\"in_channels\"] *= 2\n",
    "        self.up_blocks = nn.ModuleList([UpDownBlock(ResnetBlock(**resnet_params)) for _ in range(n_blocks + 1)])\n",
    "\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.GroupNorm(32, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            zero_init(nn.Conv2d(embedding_dim, input_channels, kernel_size=3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, g_t):\n",
    "        g_t = g_t.expand(z.shape[0])\n",
    "        t = (g_t - self.gamma_min) / (self.gamma_max - self.gamma_min)\n",
    "        t_embedding = get_timestep_embedding(t, self.embedding_dim)\n",
    "        condition = self.embed_conditioning(t_embedding)\n",
    "\n",
    "        z_in = fourier_encode(z, self.num_fourier_features) if self.use_fourier else z\n",
    "        h = self.input_conv(z_in)\n",
    "\n",
    "        skip_connections = []\n",
    "        for down_block in self.down_blocks:\n",
    "            skip_connections.append(h)\n",
    "            h = down_block(h, condition)\n",
    "\n",
    "        skip_connections.append(h)\n",
    "        h = self.mid_resnet_block_1(h, condition)\n",
    "        h = self.mid_attn_block(h)\n",
    "        h = self.mid_resnet_block_2(h, condition)\n",
    "\n",
    "        for up_block in self.up_blocks:\n",
    "            h = torch.cat([h, skip_connections.pop()], dim=1)\n",
    "            h = up_block(h, condition)\n",
    "\n",
    "        prediction = self.output_conv(h)\n",
    "        return prediction + z\n",
    "\n",
    "\n",
    "print(\"UNet model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616748d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Diffusion Model\n",
    "class VDM(nn.Module):\n",
    "    def __init__(self, model, image_shape, gamma_min=-13.3, gamma_max=5.0, device=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.image_shape = image_shape\n",
    "        self.vocab_size = 256\n",
    "        self.gamma_min = gamma_min\n",
    "        self.gamma_max = gamma_max\n",
    "        self.device = device\n",
    "\n",
    "    def gamma_schedule(self, t):\n",
    "        \"\"\"Linear gamma schedule.\"\"\"\n",
    "        return self.gamma_min + (self.gamma_max - self.gamma_min) * t\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode discrete image values to continuous latent space.\"\"\"\n",
    "        x_discrete = (x * 255).round()\n",
    "        return 2 * ((x_discrete + 0.5) / self.vocab_size) - 1\n",
    "\n",
    "    def decode(self, z, g_0):\n",
    "        \"\"\"Compute log probabilities for reconstruction.\"\"\"\n",
    "        x_vals = torch.arange(0, self.vocab_size, device=z.device, dtype=z.dtype)\n",
    "        x_vals = x_vals.view(1, 1, 1, 1, self.vocab_size)\n",
    "        x_vals_encoded = 2 * ((x_vals + 0.5) / self.vocab_size) - 1\n",
    "\n",
    "        if g_0.dim() == 0:\n",
    "            g_0 = g_0.expand(z.shape[0])\n",
    "        inv_stdev = torch.exp(-0.5 * g_0).view(-1, 1, 1, 1, 1)\n",
    "        z_expanded = z.unsqueeze(-1)\n",
    "        logits = -0.5 * torch.square((z_expanded - x_vals_encoded) * inv_stdev)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"Sample z_t ~ q(z_t | x0, t).\"\"\"\n",
    "        gamma_t = self.gamma_schedule(t)\n",
    "        gamma_t_padded = gamma_t.view(-1, 1, 1, 1)\n",
    "\n",
    "        mean = x0 * torch.sqrt(torch.sigmoid(-gamma_t_padded))\n",
    "        scale = torch.sqrt(torch.sigmoid(gamma_t_padded))\n",
    "\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "\n",
    "        z_t = mean + noise * scale\n",
    "        return z_t, noise, gamma_t\n",
    "\n",
    "    def forward(self, batch, noise=None):\n",
    "        \"\"\"Compute VDM loss for training.\"\"\"\n",
    "        x = batch\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Encode to latent space\n",
    "        f = self.encode(x)\n",
    "\n",
    "        # Get gamma values\n",
    "        g_0 = self.gamma_schedule(torch.tensor(0.0, device=x.device))\n",
    "        g_1 = self.gamma_schedule(torch.tensor(1.0, device=x.device))\n",
    "        var_0 = torch.sigmoid(g_0)\n",
    "        var_1 = torch.sigmoid(g_1)\n",
    "\n",
    "        # 1. Reconstruction loss\n",
    "        if noise is None:\n",
    "            eps_0 = torch.randn_like(f)\n",
    "        else:\n",
    "            eps_0 = noise\n",
    "        z_0 = torch.sqrt(1.0 - var_0) * f + torch.sqrt(var_0) * eps_0\n",
    "        z_0_rescaled = f + torch.exp(0.5 * g_0) * eps_0\n",
    "\n",
    "        x_discrete = (x * 255).round().long()\n",
    "        log_probs = self.decode(z_0_rescaled, g_0)\n",
    "        x_onehot = F.one_hot(x_discrete, num_classes=self.vocab_size).float()\n",
    "        log_prob = torch.sum(x_onehot * log_probs, dim=[1, 2, 3, 4])\n",
    "        loss_recon = -log_prob\n",
    "\n",
    "        # 2. KL loss\n",
    "        mean1_sqr = (1.0 - var_1) * torch.square(f)\n",
    "        loss_klz = 0.5 * torch.sum(mean1_sqr + var_1 - torch.log(var_1) - 1.0, dim=[1, 2, 3])\n",
    "\n",
    "        # 3. Diffusion loss\n",
    "        t0 = np.random.uniform(0, 1 / batch_size)\n",
    "        t = torch.arange(t0, 1.0, 1.0 / batch_size, device=self.device)[:batch_size]\n",
    "        z_t, eps, gamma_t = self.q_sample(f, t, noise=None)\n",
    "        eps_pred = self.model(z_t, gamma_t)\n",
    "        loss_diff_mse = torch.sum(torch.square(eps - eps_pred), dim=[1, 2, 3])\n",
    "        g_t_grad = self.gamma_max - self.gamma_min\n",
    "        loss_diff = 0.5 * g_t_grad * loss_diff_mse\n",
    "\n",
    "        # Total loss\n",
    "        total_loss_per_sample = loss_recon + loss_klz + loss_diff\n",
    "        total_loss = torch.mean(total_loss_per_sample)\n",
    "\n",
    "        # Convert to BPD\n",
    "        num_dims = np.prod(self.image_shape)\n",
    "        rescale_to_bpd = 1.0 / (num_dims * np.log(2.0))\n",
    "        bpd_total = torch.mean(total_loss_per_sample) * rescale_to_bpd\n",
    "\n",
    "        bpd_components = {\n",
    "            \"bpd_recon\": (torch.mean(loss_recon) * rescale_to_bpd).item(),\n",
    "            \"bpd_klz\": (torch.mean(loss_klz) * rescale_to_bpd).item(),\n",
    "            \"bpd_diff\": (torch.mean(loss_diff) * rescale_to_bpd).item(),\n",
    "        }\n",
    "\n",
    "        return total_loss, bpd_total, bpd_components\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_p_s_t(self, z, t, s, clip_samples=True):\n",
    "        \"\"\"Sample from p(z_s | z_t).\"\"\"\n",
    "        gamma_t = self.gamma_schedule(t)\n",
    "        gamma_s = self.gamma_schedule(s)\n",
    "        c = -torch.expm1(gamma_s - gamma_t)\n",
    "\n",
    "        alpha_t = torch.sqrt(torch.sigmoid(-gamma_t))\n",
    "        alpha_s = torch.sqrt(torch.sigmoid(-gamma_s))\n",
    "        sigma_t = torch.sqrt(torch.sigmoid(gamma_t))\n",
    "        sigma_s = torch.sqrt(torch.sigmoid(gamma_s))\n",
    "\n",
    "        pred_noise = self.model(z, gamma_t)\n",
    "\n",
    "        if clip_samples:\n",
    "            x_start = (z - sigma_t * pred_noise) / alpha_t\n",
    "            x_start.clamp_(-1.0, 1.0)\n",
    "            mean = alpha_s * (z * (1 - c) / alpha_t + c * x_start)\n",
    "        else:\n",
    "            mean = alpha_s / alpha_t * (z - c * sigma_t * pred_noise)\n",
    "\n",
    "        scale = sigma_s * torch.sqrt(c)\n",
    "        return mean + scale * torch.randn_like(z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, n_sample_steps=100, clip_samples=True):\n",
    "        \"\"\"Generate samples from the model.\"\"\"\n",
    "        z = torch.randn((batch_size, *self.image_shape), device=self.device)\n",
    "        steps = torch.linspace(1.0, 0.0, n_sample_steps + 1, device=self.device)\n",
    "\n",
    "        for i in range(n_sample_steps):\n",
    "            z = self.sample_p_s_t(z, steps[i], steps[i + 1], clip_samples)\n",
    "\n",
    "        # Decode final z_0 to image\n",
    "        g_0 = self.gamma_schedule(torch.tensor(0.0, device=self.device))\n",
    "        z_0_rescaled = z / torch.sqrt(torch.sigmoid(-g_0))\n",
    "        logprobs = self.decode(z_0_rescaled, g_0)\n",
    "        x = torch.argmax(logprobs, dim=-1)\n",
    "        return x.float() / (self.vocab_size - 1)\n",
    "\n",
    "\n",
    "print(\"VDM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f875d",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "image_shape = (3, 32, 32)  # CIFAR-10 image shape\n",
    "unet = UNet(embedding_dim=128, n_blocks=32, input_channels=3, use_fourier=True, num_fourier_features=7)\n",
    "vdm = VDM(unet, image_shape=image_shape, device=device)\n",
    "vdm = vdm.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in vdm.parameters())\n",
    "print(f\"Total parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if available\n",
    "checkpoint_paths = list(Path(\"outputs\").glob(\"**/model.pt\"))\n",
    "if checkpoint_paths:\n",
    "    checkpoint_path = checkpoint_paths[-1]  # Use most recent\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # Load model state\n",
    "    if \"model\" in checkpoint:\n",
    "        vdm.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "    elif \"ema\" in checkpoint and checkpoint[\"ema\"] is not None:\n",
    "        # Try loading EMA model if available\n",
    "        vdm.load_state_dict(checkpoint[\"ema\"][\"ema_model\"], strict=False)\n",
    "\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint.get('step', 'unknown')}\")\n",
    "else:\n",
    "    print(\"No checkpoint found - model has random initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff9790",
   "metadata": {},
   "source": [
    "# Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAIN_MODEL = True\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    from tqdm import tqdm\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Create fresh dataloaders for training\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=0,  # Use 0 for notebook to avoid multiprocessing issues\n",
    "    )\n",
    "\n",
    "    # Reinitialize model for training\n",
    "    unet = UNet(embedding_dim=128, n_blocks=32, input_channels=3, use_fourier=True, num_fourier_features=7)\n",
    "    vdm = VDM(unet, image_shape=image_shape, device=device)\n",
    "    vdm = vdm.to(device)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.AdamW(vdm.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.99), weight_decay=0.01, eps=1e-8)\n",
    "\n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "    print(f\"Batch size: {TRAIN_BATCH_SIZE}, Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "    # Training loop\n",
    "    training_losses = []\n",
    "    training_bpds = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        vdm.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_bpd = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for batch_idx, (images, _) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss, bpd, bpd_components = vdm(images)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vdm.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bpd += bpd.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"bpd\": f\"{bpd.item():.4f}\",\n",
    "                    \"recon\": f'{bpd_components[\"bpd_recon\"]:.3f}',\n",
    "                    \"kl\": f'{bpd_components[\"bpd_klz\"]:.3f}',\n",
    "                    \"diff\": f'{bpd_components[\"bpd_diff\"]:.3f}',\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Calculate epoch averages\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_bpd = epoch_bpd / len(train_loader)\n",
    "        training_losses.append(avg_loss)\n",
    "        training_bpds.append(avg_bpd)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, BPD: {avg_bpd:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "    # Plot training progress\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axes[0].plot(range(1, NUM_EPOCHS + 1), training_losses, marker=\"o\", linewidth=2)\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(range(1, NUM_EPOCHS + 1), training_bpds, marker=\"o\", linewidth=2, color=\"orange\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"BPD\")\n",
    "    axes[1].set_title(\"Training BPD\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training skipped - using loaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6edfb",
   "metadata": {},
   "source": [
    "# Generate Samples from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528eb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "vdm.eval()\n",
    "\n",
    "# Generate samples\n",
    "print(\"Generating samples from the model...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "num_samples = 16\n",
    "n_sample_steps = 100  # Number of denoising steps\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = vdm.sample(batch_size=num_samples, n_sample_steps=n_sample_steps, clip_samples=True)\n",
    "    samples = samples.cpu()\n",
    "\n",
    "print(f\"Generated {num_samples} samples with {n_sample_steps} denoising steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generated samples\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_samples):\n",
    "    img = samples[i].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)  # Ensure values are in [0, 1]\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Generated Samples from VDM\", fontsize=14, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show real images alongside generated ones\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3.5))\n",
    "\n",
    "# Real images (top row)\n",
    "real_images, real_labels = next(iter(test_dataloader))\n",
    "for i in range(8):\n",
    "    img = real_images[i].permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title(\"Real\", loc=\"left\", fontsize=10)\n",
    "\n",
    "# Generated images (bottom row)\n",
    "for i in range(8):\n",
    "    img = samples[i].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title(\"Generated\", loc=\"left\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples and save intermediate steps to visualize denoising\n",
    "vdm.eval()\n",
    "num_steps = 100\n",
    "steps_to_show = [0, 10, 25, 50, 75, 90, 99]\n",
    "\n",
    "z = torch.randn((1, *image_shape), device=device)\n",
    "steps = torch.linspace(1.0, 0.0, num_steps + 1, device=device)\n",
    "\n",
    "intermediate_steps = []\n",
    "with torch.no_grad():\n",
    "    for i in range(num_steps):\n",
    "        if i in steps_to_show:\n",
    "            # Decode current z to image space\n",
    "            g_t = vdm.gamma_schedule(steps[i])\n",
    "            z_rescaled = z / torch.sqrt(torch.sigmoid(-g_t))\n",
    "            intermediate_steps.append((i, z_rescaled.cpu().clone()))\n",
    "\n",
    "        z = vdm.sample_p_s_t(z, steps[i], steps[i + 1], clip_samples=True)\n",
    "\n",
    "    # Add final result\n",
    "    g_0 = vdm.gamma_schedule(torch.tensor(0.0, device=device))\n",
    "    z_0_rescaled = z / torch.sqrt(torch.sigmoid(-g_0))\n",
    "    logprobs = vdm.decode(z_0_rescaled, g_0)\n",
    "    final_img = torch.argmax(logprobs, dim=-1).float() / (vdm.vocab_size - 1)\n",
    "    intermediate_steps.append((num_steps, final_img.cpu()))\n",
    "\n",
    "# Visualize the denoising process\n",
    "fig, axes = plt.subplots(1, len(intermediate_steps), figsize=(16, 2.5))\n",
    "\n",
    "for idx, (step_num, img_tensor) in enumerate(intermediate_steps):\n",
    "    img = img_tensor[0].permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Step {step_num}\", fontsize=9)\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Denoising Process: From Noise to Image\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
